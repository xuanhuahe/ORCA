# Active Intelligence in Video Avatars via Closed-loop World Modeling 

**[Project Page](https://xuanhuahe.github.io/ORCA/)** | **[arXiv](#)**| **[Huggingface](https://huggingface.co/datasets/Alexhe101/L-IVA)**

---

## Introduction

Current video avatar generation methods excel at identity preservation and motion alignment but lack genuine agencyâ€”they cannot autonomously pursue long-term goals through adaptive environmental interaction. To address this gap, we introduce **ORCA** (Online Reasoning and Cognitive Architecture), a framework that enables active intelligence by formulating avatar control as a Partially Observable Markov Decision Process (POMDP).

ORCA treats an Image-to-Video (I2V) model as a world simulator, allowing an agent to infer the world state from observations and plan coherent action sequences to achieve high-level intentions.

---


## L-IVA Benchmark

To systematically evaluate active intelligence, we propose the **L-IVA** (Long-horizon Interactive Visual Avatar) benchmark.

**Task Composition**: Includes 100 tasks across 5 real-world scenario categories: Kitchen, Office, Garden, Livestream, and Workshop.

**Complexity**: Each task requires 3-8 interaction steps involving more than three objects, testing autonomous planning and precise control.

---

## Links

**Project Page**: [https://xuanhuahe.github.io/ORCA/](https://xuanhuahe.github.io/ORCA/) 

**arXiv**: [Link to be updated] 



---

## Code Implementation

**Code will be available soon.** 
We are currently preparing the codebase for release.


